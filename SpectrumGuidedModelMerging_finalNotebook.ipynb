{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###*Welcome to Spectrum Guided Model Merging!*"
      ],
      "metadata": {
        "id": "1p3IomsL24jX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are running tests - please use GPU (cuda) because otherwise it's really slow :)"
      ],
      "metadata": {
        "id": "7GlVEC7U0ymb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mGSMaW1qng7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive\n",
        "!git clone https://github.com/Anjaas85/Spectrum-Guided-Model-Merging.git\n",
        "%cd Spectrum-Guided-Model-Merging/"
      ],
      "metadata": {
        "collapsed": true,
        "id": "k6qHuBD4rg4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "8HC75ZMCrkc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we intend to use MergeKit following change is needed. Namely, according to LLMs, there is a error in library that has to be replaced.  "
      ],
      "metadata": {
        "id": "y0fD9EuPr42s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we can use grep to control if it's changed\n",
        "!sudo sed -i 's/\\[w_in.name\\] + (w_in.aliases or \\[\\])/[w_in.name] + list(w_in.aliases or [])/g' /usr/local/lib/python3.12/dist-packages/mergekit/plan.py"
      ],
      "metadata": {
        "id": "8n-znaW5r4Qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to prepare models to e merged, given that we are experimenting on classification tasks that are having different number of classes it is not possible to merge heads. Therefore, we are going to keep beheaded models and perserve separate heads.\n",
        "Scrypts are in ./models directory, together with new class for multi head model, which we will be exsaustively using."
      ],
      "metadata": {
        "id": "XE4lfMAGsXSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd models/\n",
        "!python3 behead.py\n",
        "!python3 tokenizer.py\n",
        "%cd .."
      ],
      "metadata": {
        "id": "GRTiKzacrsJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All of the following tests and examples, we are going to show on TIES mergining for 50% of SNR most significant layers.\\n\n",
        "\\n\n",
        "Let's start from simple merge and testing of one merged model."
      ],
      "metadata": {
        "id": "BIx8d933t746"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mergekit-yaml configs/merges/bert_ties_template_snr50.yaml ./experiments/merges/bert_ties_template_snr50_exp --cuda"
      ],
      "metadata": {
        "id": "Zw6kz8xgtHkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our model is saved and we can use our UnifiedMultiTaskModel, add classfication heads, and test it."
      ],
      "metadata": {
        "id": "ApFrGoLlvew7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoConfig, AutoModelForSequenceClassification\n",
        "from src.test_model import evaluate_model\n",
        "from models.unifiedMultiTaskModel import UnifiedMultiTaskModel\n",
        "\n",
        "merged_backbone_path = \"./experiments/merges/bert_ties_template_snr50_exp\"\n",
        "multi_model = UnifiedMultiTaskModel(merged_backbone_path)\n",
        "\n",
        "#saved different heads\n",
        "multi_model.add_task_head(\"sst2\", \"textattack/bert-base-uncased-SST-2\" )\n",
        "multi_model.add_task_head(\"ag-news\", \"textattack/bert-base-uncased-ag-news\")\n",
        "multi_model.add_task_head(\"mnli\",\"textattack/bert-base-uncased-MNLI\")\n",
        "print(\"All heads added to umtm! Everything ready!!!!\")"
      ],
      "metadata": {
        "id": "S9bElEctveNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from src.test_model import evaluate_model\n",
        "model_name = \"bert_ties_template_snr50_exp\"\n",
        "merged_path = f\"experiments/merges/{model_name}\"\n",
        "\n",
        "\n",
        "#evaluation for all tasks\n",
        "e1 = evaluate_model(multi_model,model_name,\"sst2\")\n",
        "e2 = evaluate_model(multi_model, model_name, \"ag-news\")\n",
        "e3 = evaluate_model(multi_model, model_name, \"mnli\")\n",
        "print(e1)\n",
        "print(e2)\n",
        "print(e3)\n",
        "print(\"All checked! :)\")"
      ],
      "metadata": {
        "id": "mxbS5YtewOaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's how we were testing toy models, then it was necessary to find the best parameters."
      ],
      "metadata": {
        "id": "-SCJ5rkWwxaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from src.optimize_ties_merge_snr50 import optimize_ties_params\n",
        "\n",
        "best_params = optimize_ties_params(\n",
        "    template_config_path=\"./configs/merges/bert_ties_template_snr50.yaml\",\n",
        "    output_dir=\"./experiments/optuna_ties_snr50\",\n",
        "    n_trials=148, # 149 if you want to play one trial - there is 148 trials in database\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "print(f'\\n I suppose we are done :)')\n"
      ],
      "metadata": {
        "id": "ttO0zcIkww7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we have the parameters needed we can run optimal merge and finetune it!"
      ],
      "metadata": {
        "id": "P_CrcLUVycdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mergekit-yaml configs/merges/merges_highlights/bert_ties_snr50_opt.yaml ./experiments/merges/bert_ties_snr50_opt --cuda"
      ],
      "metadata": {
        "id": "anM2I5j7yYNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FINE-TUNE THE SPECTRUM MERGED MODELS\n",
        "import torch\n",
        "from models.unifiedMultiTaskModel import UnifiedMultiTaskModel\n",
        "from src.fine_tune import fine_tune_and_evaluate\n",
        "\n",
        "# only the spectrum guided ones: \"bert_ties_snr50_opt\", itd..\n",
        "model_name = \"bert_ties_snr50_opt\"\n",
        "merged_path = f\"./experiments/merges/{model_name}\"\n",
        "\n",
        "print(f\"Processing Model: {model_name}\")\n",
        "print(f\"Path: {merged_path}\")\n",
        "\n",
        "multi_model = UnifiedMultiTaskModel(merged_path)\n",
        "\n",
        "multi_model.add_task_head(\"sst2\", \"textattack/bert-base-uncased-SST-2\")\n",
        "multi_model.add_task_head(\"ag-news\", \"textattack/bert-base-uncased-ag-news\")\n",
        "multi_model.add_task_head(\"mnli\", \"textattack/bert-base-uncased-MNLI\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "fine_tune_and_evaluate(\n",
        "    multitask_model_instance=multi_model,\n",
        "    model_name=model_name,\n",
        "    output_run_name=model_name+\"ft\", #ft for fine-tuned\n",
        "    total_steps=3000, #you can run on 2 steps to try\n",
        "    base_lr=2e-5,\n",
        "    device=device,\n",
        "    val_check_interval=300,  # if you are running on 2 stpes put it to 1\n",
        "    arr_threshold=0.98,\n",
        "    patience=4,\n",
        "    train_subset_ratio=0.1\n",
        ")\n",
        "\n",
        "print(\"\\nFine Tuning and Evaluation Complete :)\")"
      ],
      "metadata": {
        "id": "Lnv-y1Nawg78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*That's it! Our short example notebook is over!*\n"
      ],
      "metadata": {
        "id": "H8_ZdzMD1GRx"
      }
    }
  ]
}